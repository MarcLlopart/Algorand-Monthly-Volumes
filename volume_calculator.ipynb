{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/marc/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import yfinance as yf\n",
    "\n",
    "\n",
    "import requests\n",
    "from datetime import datetime, timedelta, timezone\n",
    "\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "MONTH_DIR = 'May 2025'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stables\n",
    "# 31566704 USDC\n",
    "#Â 312769 USDT \n",
    "#Â 227855942 EURS\n",
    "# 760037151 xUSD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_csv(coin_id, currency):\n",
    "    \"\"\"\n",
    "    This function downloads the data from Coingecko with the usd pair.\n",
    "    Inputs:\n",
    "        - coin_id: the name of the crypto we are getting the data\n",
    "        - currency: by default it is the usd\n",
    "    Output:\n",
    "        - This function simply stores under raw_data directory a file\n",
    "        from coingecko with all historic data available\n",
    "    \"\"\"\n",
    "    # Construct the URL\n",
    "    base_url = \"https://www.coingecko.com\"\n",
    "    csv_path = f\"/price_charts/export/{coin_id}/{currency}.csv\"\n",
    "    full_url = base_url + csv_path\n",
    "\n",
    "    # Download the CSV\n",
    "    response = requests.get(full_url)\n",
    "    if response.status_code == 200:\n",
    "        file_name = f\"{MONTH_DIR}/{coin_id}_historical_data.csv\"\n",
    "        with open(file_name, \"wb\") as file:\n",
    "            file.write(response.content)\n",
    "\n",
    "    else:\n",
    "        print(f\"Failed to download CSV for {coin_id}.\")\n",
    "\n",
    "download_csv('algorand', 'usd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_to_unix_timestamp(start_date_str, end_date_str):\n",
    "  \"\"\"\n",
    "  Converts start and end dates in YYYY-MM-DD format to Unix timestamps.\n",
    "\n",
    "  Args:\n",
    "    start_date_str: Start date string in YYYY-MM-DD format.\n",
    "    end_date_str: End date string in YYYY-MM-DD format.\n",
    "\n",
    "  Returns:\n",
    "    A tuple containing the start and end Unix timestamps.\n",
    "  \"\"\"\n",
    "\n",
    "  start_date = datetime.strptime(start_date_str, \"%Y-%m-%d\").replace(tzinfo=timezone.utc)\n",
    "  end_date = datetime.strptime(end_date_str, \"%Y-%m-%d\").replace(tzinfo=timezone.utc) + timedelta(days=1) - timedelta(seconds=1) \n",
    "\n",
    "  start_timestamp = int(start_date.timestamp())\n",
    "  end_timestamp = int(end_date.timestamp())\n",
    "\n",
    "  return start_timestamp, end_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_close_price(start_date, end_date, asset_id):\n",
    "    \"\"\"\n",
    "    Fetches an asset historical data from the API for the given interval.\n",
    "    \n",
    "    Args:\n",
    "        start_timestamp: start unix timestamp\n",
    "        end_timestamp: end unix timestamp\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Dataframe containing the fetched data in daily intervals.\n",
    "    \"\"\"\n",
    "    start_unix, end_unix = date_to_unix_timestamp(start_date, end_date)\n",
    "\n",
    "    price_feed = f'https://indexer.vestige.fi/assets/{asset_id}/candles?network_id=0&interval=86400&start={start_unix}&end={end_unix}&denominating_asset_id=0&volume_in_denominating_asset=false'\n",
    "\n",
    "    response = requests.get(price_feed)\n",
    "    data = response.json()\n",
    "    df = pd.DataFrame(data)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = '2025-05-01'\n",
    "end_date = '2025-05-31'\n",
    "\n",
    "stables_dfs = {}\n",
    "stables_ids = {'usdc': 31566704, 'usdt': 312769,\n",
    "              'eurs': 227855942,'xusd':760037151}\n",
    "\n",
    "for stablecoin, stables_id in stables_ids.items():\n",
    "    stables_dfs[stablecoin] = get_close_price(start_date, end_date, stables_id)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = '2025-05-01'\n",
    "end_date = '2025-05-31'\n",
    "\n",
    "asas_dfs = {}\n",
    "asset_ids = {\n",
    "    'ora': 1284444444, 'talgo': 2537013734, 'niko': 1265975021, 'vote': 452399768, 'tiny': 2200000000,\n",
    "    'alpha': 2726252423, 'monko': 2494786278, 'gobtc': 386192725, 'ac': 2317534335, 'opul': 287867876,\n",
    "    'xalgo': 1134696561, 'coop': 796425061, 'chip': 388592191, 'malgo': 1185173782, 'avoi': 2320775407, \n",
    "    'arbz': 1285492943, 'gora': 1138500612, 'pepe': 1096015467, 'fish': 2667006169, 'goeth': 386195940\n",
    "}\n",
    "\n",
    "for asa, asa_id in asset_ids.items():\n",
    "    asas_dfs[asa] = get_close_price(start_date, end_date, asa_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'dt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'dt'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 25\u001b[0m\n\u001b[1;32m     22\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m combined_df\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m stables_final_df \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_combined_df\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstables_dfs\u001b[49m\u001b[43m)\u001b[49m \n\u001b[1;32m     26\u001b[0m asas_final_df \u001b[38;5;241m=\u001b[39m create_combined_df(asas_dfs)\n",
      "Cell \u001b[0;32mIn[8], line 17\u001b[0m, in \u001b[0;36mcreate_combined_df\u001b[0;34m(dataframes)\u001b[0m\n\u001b[1;32m     14\u001b[0m combined_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame()\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m asset, df \u001b[38;5;129;01min\u001b[39;00m dataframes\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m---> 17\u001b[0m   df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m, unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m'\u001b[39m) \n\u001b[1;32m     18\u001b[0m   df\u001b[38;5;241m.\u001b[39mset_index(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m'\u001b[39m, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \n\u001b[1;32m     19\u001b[0m   df\u001b[38;5;241m.\u001b[39mrename(columns\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclose\u001b[39m\u001b[38;5;124m'\u001b[39m: asset}, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3810\u001b[0m     ):\n\u001b[1;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'dt'"
     ]
    }
   ],
   "source": [
    "def create_combined_df(dataframes):\n",
    "  \"\"\"\n",
    "  Creates a single DataFrame with 'date' as the index and \n",
    "  'close' columns named after the stablecoin keys.\n",
    "\n",
    "  Args:\n",
    "    dataframes: A dictionary where keys are stablecoin names \n",
    "                and values are the corresponding DataFrames.\n",
    "\n",
    "  Returns:\n",
    "    A pandas DataFrame with the combined data.\n",
    "  \"\"\"\n",
    "\n",
    "  combined_df = pd.DataFrame()\n",
    "\n",
    "  for asset, df in dataframes.items():\n",
    "    df['date'] = pd.to_datetime(df['dt'], unit='s') \n",
    "    df.set_index('date', inplace=True) \n",
    "    df.rename(columns={'close': asset}, inplace=True) \n",
    "    combined_df = combined_df.join(df[asset], how='outer') \n",
    "\n",
    "  return combined_df\n",
    "\n",
    "# Example usage\n",
    "stables_final_df = create_combined_df(stables_dfs) \n",
    "asas_final_df = create_combined_df(asas_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "stables_final_df.to_csv(f'{MONTH_DIR}/Results/stablecoins_24.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "asas_final_df.to_csv(f'{MONTH_DIR}/Results/asas_jan.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo_price = pd.read_csv(f'{MONTH_DIR}/algorand_historical_data.csv')\n",
    "algo_price['snapped_at'] = pd.to_datetime(algo_price['snapped_at'])\n",
    "price_feed = algo_price[(algo_price['snapped_at'] >= start_date) & (algo_price['snapped_at'] <= end_date)]\n",
    "price_feed[['snapped_at', 'price']].to_csv(f'{MONTH_DIR}/Results/ALGO_dec.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "asa_volumes = pd.read_csv(f'{MONTH_DIR}/asa_volumes.csv')\n",
    "asa_volumes['dt'] = pd.to_datetime(asa_volumes['dt'])\n",
    "asa_volumes.columns.values[1:]\n",
    "asas_final_df.reset_index(inplace=True)\n",
    "asas_final_df['date'] = pd.to_datetime(asas_final_df['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>index</th>\n",
       "      <th>date</th>\n",
       "      <th>ora</th>\n",
       "      <th>niko</th>\n",
       "      <th>talgo</th>\n",
       "      <th>vote</th>\n",
       "      <th>tiny</th>\n",
       "      <th>gobtc</th>\n",
       "      <th>silver</th>\n",
       "      <th>...</th>\n",
       "      <th>monko</th>\n",
       "      <th>opul</th>\n",
       "      <th>gold</th>\n",
       "      <th>chip</th>\n",
       "      <th>malgo</th>\n",
       "      <th>coop</th>\n",
       "      <th>avoi</th>\n",
       "      <th>goeth</th>\n",
       "      <th>daffir</th>\n",
       "      <th>calgo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows Ã 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [date, index, date, ora, niko, talgo, vote, tiny, gobtc, silver, rio, stbl, alpha, monko, opul, gold, chip, malgo, coop, avoi, goeth, daffir, calgo]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 23 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "asa_volumes = pd.read_csv(f'{MONTH_DIR}/asa_volumes.csv')\n",
    "asa_volumes['dt'] = pd.to_datetime(asa_volumes['dt'])\n",
    "\n",
    "\n",
    "def multiply_columns(df1, df2):\n",
    "  \"\"\"\n",
    "  Joins price and volume dataframes to convert into ALGO volumes\n",
    "\n",
    "  Args:\n",
    "    df1: Volumes Dataframe in native Currency\n",
    "    df2: Asset/ALGO Price DataFrame.\n",
    "\n",
    "  Returns:\n",
    "    A merged DataFrame with ALGO Volumes\n",
    "  \"\"\"\n",
    "\n",
    "  # Merge DataFrames on 'dt' and 'date'\n",
    "  merged_df = df1.merge(df2, left_on='dt', right_on='date', suffixes=('_df1', '_df2'))\n",
    "\n",
    "  # Calculate product of corresponding columns\n",
    "  columns_to_multiply = df1.columns.values[1:]\n",
    "  for col in columns_to_multiply:\n",
    "      merged_df[col] = merged_df[col + '_df1'] * merged_df[col + '_df2']\n",
    "\n",
    "  # Drop unnecessary columns\n",
    "  merged_df.drop(columns=[col + '_df1' for col in columns_to_multiply] + \n",
    "                  [col + '_df2' for col in columns_to_multiply], \n",
    "             inplace=True)\n",
    "\n",
    "  # Rename 'dt' column to 'date'\n",
    "  merged_df.rename(columns={'dt': 'date'}, inplace=True)\n",
    "\n",
    "  return merged_df\n",
    "\n",
    "\n",
    "result_df = multiply_columns(asa_volumes, asas_final_df)\n",
    "\n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def price_merge(algo_volumes, algo_price):\n",
    "  \"\"\"\n",
    "  Joins volumes and algo price dataframes to convert into USD volume\n",
    "\n",
    "  Args:\n",
    "    df1: Volumes Dataframe in ALGO\n",
    "    df2: ALGO/USD Price DataFrame.\n",
    "\n",
    "  Returns:\n",
    "    A merged DataFrame with ALGO Volumes\n",
    "  \"\"\"\n",
    "\n",
    "    # Merge DataFrames on 'dt' and 'date'\n",
    "  merged_df = algo_volumes.merge(algo_price[['snapped_at', 'price']], left_on='date', right_on='snapped_at')\n",
    "\n",
    "  merged_df.drop('snapped_at', axis=1, inplace=True)\n",
    "  # Calculate product of corresponding columns\n",
    "  merged_df['usd_vol'] = merged_df['algo_vol']*merged_df['price']\n",
    "\n",
    "  # Rename 'dt' column to 'date'\n",
    "  #merged_df.rename(columns={'dt': 'date'}, inplace=True)\n",
    "\n",
    "  return merged_df\n",
    "\n",
    "\n",
    "stables = pd.read_csv(f'{MONTH_DIR}/stables.csv')\n",
    "algorand = pd.read_csv(f'{MONTH_DIR}/algorand_historical_data.csv')\n",
    "\n",
    "stables['dt'] = pd.to_datetime(stables['dt'])\n",
    "algorand['snapped_at'] = pd.to_datetime(algorand['snapped_at']).dt.date\n",
    "\n",
    "result_df = multiply_columns(stables, stables_final_df)\n",
    "result_df['algo_vol'] = result_df[result_df.columns.values[1:]].sum(axis=1)\n",
    "\n",
    "result_df['date'] = pd.to_datetime(result_df['date']).dt.date\n",
    "\n",
    "usd = price_merge(result_df, algorand)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.to_csv(f'{MONTH_DIR}/Results/asas_algo_volumes.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "# HAFN\n",
    "\n",
    "def get_price_feed(ticker):\n",
    "  \"\"\"\n",
    "  Fetches the price feed for the specified ticker for December 2024 \n",
    "  using the yfinance library.\n",
    "\n",
    "  Args:\n",
    "    ticker: The ticker symbol (e.g., \"AFNUSD=X\").\n",
    "\n",
    "  Returns:\n",
    "    A pandas DataFrame containing the historical data for the specified period.\n",
    "  \"\"\"\n",
    "\n",
    "  try:\n",
    "    # Define start and end dates, consider forex does not operate 24/7\n",
    "    start_date = \"2025-01-01\"\n",
    "    end_date = \"2025-01-31\"\n",
    "\n",
    "    # Download historical data using yfinance\n",
    "    data = yf.download(ticker, start=start_date, end=end_date)\n",
    "    return data\n",
    "  except Exception as e:\n",
    "    print(f\"Error fetching data for {ticker}: {e}\")\n",
    "    return None\n",
    "\n",
    "# Example usage\n",
    "ticker = \"AFNUSD=X\"\n",
    "december_data = get_price_feed(ticker)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "december_data.to_csv(f'{MONTH_DIR}/Results/HAFN_dec.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
